{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T07:43:25.201443Z",
     "start_time": "2025-04-23T07:43:21.287065800Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/root/ProtAgent\")\n",
    "import os\n",
    "import yaml\n",
    "import random\n",
    "import json\n",
    "\n",
    "from agent.utils.others import setup_seed\n",
    "from agent.tools.tool_manager import ToolManager\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from agent.agent.multi_agent_backbone import ProteinReActAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b4cd63bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_manager = ToolManager(enable_quick_run=True)\n",
    "tool_manager.set_out_dir(\"/home/public/ProtAgent/examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e38bc2b3e4592b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load all available tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "77e2405ebdbed675",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:21:35.320237Z",
     "start_time": "2025-04-09T09:21:34.961650900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tool_dir = \"/root/ProtAgent/agent/tools\"\n",
    "tools = {}\n",
    "input_type2tool = {}\n",
    "output_type2tool = {}\n",
    "\n",
    "exclude_list = [\n",
    "    \"protrek_text2text\",\n",
    "    \"evolla\",\n",
    "]\n",
    "\n",
    "for tool_name, tool_cls in tool_manager.tools.items():\n",
    "    config = tool_cls.config\n",
    "    \n",
    "    # Load tool document\n",
    "    docs = config[\"document\"]\n",
    "    if isinstance(docs, dict):\n",
    "        docs = [docs]\n",
    "    \n",
    "    for tool_doc in docs:\n",
    "        if tool_doc[\"tool_name\"] in exclude_list:\n",
    "            continue\n",
    "        \n",
    "        if tool_doc[\"tool_name\"] not in tool_manager.tools.keys():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Record input types\n",
    "            input_type_cnt = {}\n",
    "            input_types = []\n",
    "            for param in tool_doc[\"required_parameters\"]:\n",
    "                detailed_type = param[\"detailed_type\"]\n",
    "                # Count the number of the same type\n",
    "                input_type_cnt[detailed_type] = input_type_cnt.get(detailed_type, 0) + 1\n",
    "                input_types.append(f\"{detailed_type}_{input_type_cnt[detailed_type]}\")\n",
    "                \n",
    "            # Record output types    \n",
    "            output_type_cnt = {}\n",
    "            output_types = []\n",
    "            for param in tool_doc[\"return_values\"]:\n",
    "                detailed_type = param[\"detailed_type\"]\n",
    "                # Count the number of the same type\n",
    "                output_type_cnt[detailed_type] = output_type_cnt.get(detailed_type, 0) + 1\n",
    "                output_types.append(f\"{detailed_type}_{output_type_cnt[detailed_type]}\")\n",
    "            \n",
    "            tools[tool_doc[\"tool_name\"]] = {\n",
    "                \"input_types\": input_types,\n",
    "                \"output_types\": output_types,\n",
    "                \"description\": tool_doc[\"tool_description\"],\n",
    "            }\n",
    "            \n",
    "            for input_type in input_types:\n",
    "                if input_type not in input_type2tool:\n",
    "                    input_type2tool[input_type] = []\n",
    "                input_type2tool[input_type].append(tool_doc[\"tool_name\"])\n",
    "            \n",
    "            for output_type in output_types:\n",
    "                if output_type not in output_type2tool:\n",
    "                    output_type2tool[output_type] = []\n",
    "                output_type2tool[output_type].append(tool_doc[\"tool_name\"])\n",
    "        \n",
    "        except Exception as e:\n",
    "            # print(dir_name, e)\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b2363f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA_SEQUENCE_1\n",
      "TEXT_1\n",
      "FASTA_FILE_LIST_1\n",
      "FASTA_PATH_1\n",
      "AA_SEQUENCE_2\n",
      "STRUCTURE_PATH_1\n",
      "AA_POSITION_1\n",
      "AA_POSITION_2\n",
      "CHAIN_ID_1\n",
      "CLUSTALW_ALN_PATH_1\n",
      "HMMER_HMM_PATH_1\n",
      "HHSUITE_HMM_PATH_1\n",
      "HHSUITE_HMM_PATH_2\n",
      "HHSUITE_A3M_PATH_1\n",
      "HHSUITE_A3M_PATH_2\n",
      "PDB_ID_1\n",
      "PFAM_ID_1\n",
      "UNIPROT_ID_1\n",
      "FOLDSEEK_SEQUENCE_1\n",
      "RFDIFFUSION_CONTIGS_1\n",
      "LABEL_NUM_1\n",
      "ADAPTOR_DIRECTORY_1\n",
      "MUTATION_INFO_1\n",
      "TRAINING_DATASET_1\n",
      "STRUCTURE_PATH_2\n",
      "SMILES_1\n",
      "UNIPROT_KEYWORD_1\n"
     ]
    }
   ],
   "source": [
    "for input_type in input_type2tool.keys():\n",
    "    print(input_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cd4aacd7d1d86f80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:23:47.006713Z",
     "start_time": "2025-04-09T11:23:46.903035500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "type2case = {\n",
    "    \"HHSUITE_A3M_PATH_1\": \"example_1.a3m\",\n",
    "    \"FASTA_PATH_1\": \"example_1.fasta\",\n",
    "    \"FASTA_FILE_LIST_1\": [\"class_0.fasta\", \"class_1.fasta\"],\n",
    "    \"AA_SEQUENCE_1\": \"MSATAEQNARNPKGKGGFARTVSQRKRKRLFLIGGALAVLAVAVGLMLTAFNQDIRFFRTPADLTEQDMTSGARFRLGGLVEEGSVSRTGSELRFTVTDTIKTVKVVFEGIPPDLFREGQGVVAEGRFGSDGLFRADNVLAKHDENYVPKDLADSLKKKGVWEGK\",\n",
    "    \"AA_SEQUENCE_2\": \"MITLDWEKANGLITTVVQDATTKQVLMVAYMNQESLAKTMATGETWFWSRSRKTLWHKGATSGNIQTVKTIAVDCDADTLLVTVDPAGPACHTGHISCFYRHYPEGKDLT\",\n",
    "    \"STRUCTURE_PATH_1\": \"example_1.pdb\",\n",
    "    \"STRUCTURE_PATH_2\": \"example_2.pdb\",\n",
    "    \"FOLDSEEK_SEQUENCE_1\": \"dddddddddddddddpdpppvcppvnvvvvvvvvvvvvvvvvvvvvvvvvvvqdpqdedeqvrddpcqqpvqhkhkykafwappqwdddpqkiwtwghnppgiaieieghdappqddhrfikifiaghdpvrhtygdhidtdddpddddvvnvvvcvvvvndpdd\",\n",
    "    \"TEXT_1\": \"Catalyzes the hydrolysis of cutin, a polyester that forms the structure of plant cuticle.\",\n",
    "    \"RFDIFFUSION_CONTIGS_1\": \"A:50\",\n",
    "    \"MUTATION_INFO_1\": \"A123B:C124D\",\n",
    "    \"SMILES_1\": \"CC(=O)OC1=C(C(=C(C=C1)C(=O)O)C(=O)O)C(=O)O\",\n",
    "    \"PDB_ID_1\": \"1A2B\",\n",
    "    \"HHSUITE_HMM_PATH_1\": \"example.hmm\",\n",
    "    \"UNIPROT_ID_1\": \"P05067\",\n",
    "    \"PFAM_ID_1\": \"PF00085\",\n",
    "    \"UNIPROT_KEYWORD_1\": \"cutinase\",\n",
    "    \"AA_POSITION_1\": \"5\",\n",
    "    \"AA_POSITION_2\": \"10\",\n",
    "    \"CHAIN_ID_1\": \"A\",\n",
    "    \"CLUSTALW_ALN_PATH_1\": \"result.aln\",\n",
    "    \"HMMER_HMM_PATH_1\": \"example_hmmer.hmm\",\n",
    "    \"HHSUITE_HMM_PATH_2\": \"example_hhsuite.hmm\",\n",
    "    \"HHSUITE_A3M_PATH_2\": \"example_2.a3m\",\n",
    "    \"LABEL_NUM_1\": \"2\",\n",
    "    \"ADAPTOR_DIRECTORY_1\": \"adaptor_directory\",\n",
    "    \"TRAINING_DATASET_1\": \"experiment_results.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6596278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for type2case_key in type2case.keys():\n",
    "    if type2case_key not in input_type2tool.keys():\n",
    "        print(type2case_key)\n",
    "\n",
    "for input_type in input_type2tool.keys():\n",
    "    if input_type not in type2case.keys():\n",
    "        print(input_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984cd67f5849b74e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Sample tool calling trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b77aaf1d316c77ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T10:06:05.086616800Z",
     "start_time": "2025-04-09T10:06:05.036566400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seq2fasta -> hmmsearch -> hhblits -> hhalign_msa': {'tool_chain': ['seq2fasta', 'hmmsearch', 'hhblits', 'hhalign_msa'], 'required_inputs': {'HHSUITE_A3M_PATH_2', 'HMMER_HMM_PATH_1', 'AA_SEQUENCE_1'}}, 'esmfold -> rfdiffusion_motif_scaffolding -> pdb2aaseq -> deepab -> diffab_optimize': {'tool_chain': ['esmfold', 'rfdiffusion_motif_scaffolding', 'pdb2aaseq', 'deepab', 'diffab_optimize'], 'required_inputs': {'RFDIFFUSION_CONTIGS_1', 'AA_SEQUENCE_1', 'AA_SEQUENCE_2'}}, 'protrek_structure2protein -> seq2fasta -> hmmsearch -> fasta2seq -> saprot_tuned_inference_classification': {'tool_chain': ['protrek_structure2protein', 'seq2fasta', 'hmmsearch', 'fasta2seq', 'saprot_tuned_inference_classification'], 'required_inputs': {'LABEL_NUM_1', 'HMMER_HMM_PATH_1', 'FOLDSEEK_SEQUENCE_1', 'ADAPTOR_DIRECTORY_1'}}, 'biorxiv -> pinal -> saprot_tuned_inference_classification': {'tool_chain': ['biorxiv', 'pinal', 'saprot_tuned_inference_classification'], 'required_inputs': {'LABEL_NUM_1', 'TEXT_1', 'ADAPTOR_DIRECTORY_1'}}, 'rfdiffusion_motif_scaffolding -> proteinmpnn -> saprot_mutation_byinfo': {'tool_chain': ['rfdiffusion_motif_scaffolding', 'proteinmpnn', 'saprot_mutation_byinfo'], 'required_inputs': {'RFDIFFUSION_CONTIGS_1', 'MUTATION_INFO_1', 'STRUCTURE_PATH_1'}}, 'interproscan': {'tool_chain': ['interproscan'], 'required_inputs': {'FASTA_PATH_1'}}, 'pfam_entry -> pubmed -> biorxiv -> protrek_text2structure -> uniprot_fetch_byid -> saprot_tuned_inference_classification': {'tool_chain': ['pfam_entry', 'pubmed', 'biorxiv', 'protrek_text2structure', 'uniprot_fetch_byid', 'saprot_tuned_inference_classification'], 'required_inputs': {'LABEL_NUM_1', 'PFAM_ID_1', 'ADAPTOR_DIRECTORY_1'}}, 'seq2fasta -> fasta2seq -> saprot_tuned_inference_token_classification': {'tool_chain': ['seq2fasta', 'fasta2seq', 'saprot_tuned_inference_token_classification'], 'required_inputs': {'LABEL_NUM_1', 'AA_SEQUENCE_1', 'ADAPTOR_DIRECTORY_1'}}, 'rfdiffusion_partial_diffusion -> foldseek_search -> pdb_entry -> pdb2aaseq -> protrek_protein2protein -> mmseqs_msa': {'tool_chain': ['rfdiffusion_partial_diffusion', 'foldseek_search', 'pdb_entry', 'pdb2aaseq', 'protrek_protein2protein', 'mmseqs_msa'], 'required_inputs': {'RFDIFFUSION_CONTIGS_1', 'STRUCTURE_PATH_1'}}, 'pinal -> esmfold -> tmalign': {'tool_chain': ['pinal', 'esmfold', 'tmalign'], 'required_inputs': {'TEXT_1', 'STRUCTURE_PATH_2'}}}\n"
     ]
    }
   ],
   "source": [
    "setup_seed(250728)\n",
    "\n",
    "# Decide the number of steps\n",
    "step = 6\n",
    "num = 10\n",
    "cases = {}\n",
    "\n",
    "while len(cases) < num:\n",
    "    # Randomly sample the initial tool\n",
    "    init_tool = random.choice(list(tools.keys()))\n",
    "    # init_tool = \"uniprot_query\"\n",
    "    \n",
    "    tool_chain = [init_tool]\n",
    "    # Inputs that the user has to provide\n",
    "    required_inputs = set(tools[init_tool][\"input_types\"])\n",
    "    # Inputs that later steps can obtain\n",
    "    available_inputs = required_inputs.union(set(tools[init_tool][\"output_types\"]))\n",
    "    \n",
    "    for i in range(step-1):\n",
    "        # Obtain available tools for the next step\n",
    "        output_types = tools[tool_chain[-1]][\"output_types\"]\n",
    "        next_tools = []\n",
    "        for output_type in output_types:\n",
    "            if output_type in input_type2tool:\n",
    "                next_tools.extend(input_type2tool[output_type])\n",
    "        \n",
    "        # If no next tool is available, break the loop\n",
    "        if next_tools == []:\n",
    "            break\n",
    "        \n",
    "        # Randomly sample the next tool (can be the same as the previous one)\n",
    "        while True:\n",
    "            next_tool = random.choice(next_tools)\n",
    "            if next_tool not in tool_chain:\n",
    "                break\n",
    "        tool_chain.append(next_tool)\n",
    "        \n",
    "        # Update the required inputs and available inputs\n",
    "        new_required_inputs = set(tools[next_tool][\"input_types\"]) - available_inputs\n",
    "        required_inputs = required_inputs.union(new_required_inputs)\n",
    "        available_inputs = available_inputs.union(set(tools[next_tool][\"output_types\"]))\n",
    "    \n",
    "    # If some required inputs are not provided, skip this tool chain\n",
    "    if required_inputs - type2case.keys() != set():\n",
    "        continue\n",
    "    \n",
    "    else:\n",
    "        tool_order = \" -> \".join(tool_chain)\n",
    "        cases[tool_order] = {\n",
    "            \"tool_chain\": tool_chain,\n",
    "            \"required_inputs\": required_inputs,\n",
    "        }\n",
    "\n",
    "\n",
    "print(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318721f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['protrek_structure2structure -> uniprot_fetch_byid -> saprot_mutation_bypos', 'proteinmpnn -> seq2fasta -> hmmsearch -> hhblits -> hhalign_msa', 'clustalw -> hmmbuild -> hmmsearch -> hmmscan -> hhblits -> hhfilter', 'clustalw -> hmmbuild -> hmmsearch -> hhblits -> hhalign_msa', 'umol -> pdb2aaseq -> saprot_mutation_bypos', 'uniprot_query -> hhblits -> hhalign_msa', 'saprot_tune_regression -> saprot_tuned_inference_classification', 'saprot_tuned_inference_token_classification', 'hmmsearch -> fasta2seq -> saprot_tuned_inference_regression', 'alphafold2 -> extract_peptide -> blast -> fasta2seq -> saprot_tuned_inference_classification', 'foldseek -> protrek_structure2structure -> protrek_structure2protein -> saprot_mutation_byinfo', 'saprot_tuned_inference_classification', 'protrek_text2protein -> umol -> pdb2aaseq -> esmfold -> foldseek -> protrek_structure2text', 'diffab_design', 'hhalign_msa', 'hmmscan -> clustalw -> hmmbuild -> hmmsearch -> hhblits -> hhfilter', 'tmalign', 'pfam_entry -> pubmed -> protrek_text2structure -> protrek_structure2structure -> pfam_match -> wikipedia', 'saprot_tune_regression -> saprot_tuned_inference_token_classification', 'interproscan', 'saprot_mutation_byinfo', 'uniprot_fetch_byid -> saprot_tuned_inference_regression', 'diffab_optimize', 'clustalw -> hmmbuild -> hmmsearch -> hhblits -> hhmake', 'uniprot_query -> fasta2seq -> saprot_mutation_byinfo', 'saprot_tune_pair_classification -> saprot_tuned_inference_regression', 'blast -> mmseqs_cluster -> hmmscan -> interproscan', 'pubmed -> protrek_text2protein -> protrek_protein2structure -> protrek_structure2protein -> saprot_mutation_byinfo', 'umol -> get_chain_sequence -> deepab -> foldseek -> protrek_structure2protein -> seq2fasta', 'rfdiffusion_motif_scaffolding -> extract_peptide -> foldseek_search -> pdb_entry -> extract_backbone -> diffab_design', 'rfdiffusion_binder_design -> get_protein_length', 'pdb2aaseq -> esmfold -> foldseek_search -> pdb_entry -> proteinmpnn -> deepab', 'chat -> protrek_text2structure -> protrek_structure2protein -> uniprot_fetch_byid -> mmseqs_msa -> hhfilter', 'pubmed -> protrek_text2protein -> uniprot_fetch_byid -> mmseqs_msa -> hhmake', 'protrek_text2protein -> umol -> get_chain_sequence -> seq2fasta -> hmmscan -> hmmsearch', 'umol -> rfdiffusion_motif_scaffolding -> extract_peptide -> blast -> clustalw -> hmmbuild', 'hhsearch -> hhmake', 'protrek_text2structure -> pfam_match -> pubmed -> protrek_text2protein -> alphafold2 -> extract_peptide', 'mmseqs_msa -> hhmake', 'hhsearch -> hhfilter -> hhmake', 'hhfilter -> hhalign_msa', 'seq2fasta -> hmmscan -> hhblits -> hhfilter -> hhmake', 'hhmake', 'foldseek_search -> pdb_entry -> rfdiffusion_binder_design -> get_protein_length', 'protrek_protein2structure -> protrek_structure2protein -> seq2fasta -> mmseqs_cluster -> clustalw -> hmmbuild', 'pdb_entry -> diffab_optimize', 'clustalw -> hmmbuild -> hmmsearch -> hhblits -> hhfilter -> hhalign_msa', 'pinal -> interproscan', 'saprot_mutation_bypos', 'foldseek -> protrek_structure2protein -> protrek_protein2structure -> protrek_structure2text -> protrek_text2structure -> pfam_match', 'hhalign_hmm', 'pubmed -> pinal -> protrek_protein2protein -> deepab -> rfdiffusion_motif_scaffolding -> diffab_design', 'pdb2aaseq -> mmseqs_msa -> hhalign_msa'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ea86c79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cbc51f5a2b0af0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Generate user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6b07876fa5f7e798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:56:47.388675200Z",
     "start_time": "2025-04-18T06:56:47.326813700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Record how can a input type be converted to another type by a tool\n",
    "transfer_matrix = {}\n",
    "for tool_name, obj in tool_manager.tools.items():\n",
    "    doc = obj.config.document\n",
    "    input_types = set([param[\"detailed_type\"] for param in doc.required_parameters])\n",
    "    output_types = set([param[\"detailed_type\"] for param in doc.return_values])\n",
    "    \n",
    "    if len(input_types) == 1:\n",
    "        input_type = input_types.pop()\n",
    "        # Each output type can be generated given the input type and the tool\n",
    "        if input_type not in transfer_matrix:\n",
    "            transfer_matrix[input_type] = {output_type: [tool_name] for output_type in output_types}\n",
    "        \n",
    "        else:\n",
    "            for output_type in output_types:\n",
    "                transfer_matrix[input_type][output_type] = transfer_matrix[input_type].get(output_type, []) + [tool_name]\n",
    "    \n",
    "\n",
    "def find_path(input_type: str, output_type: str, exclusive: set = None) -> list:\n",
    "    \"\"\"\n",
    "    Find the shortest paths from input_type to output_type in the transfer matrix.\n",
    "    Args:\n",
    "        input_type: Input type\n",
    "        output_type: Output type\n",
    "\n",
    "    Returns:\n",
    "        A list of tool chains that can convert the input_type to output_type.\n",
    "    \"\"\"\n",
    "    assert input_type != output_type, f\"input type and output type are the same: {input_type}\"\n",
    "    \n",
    "    if input_type not in transfer_matrix:\n",
    "        return []\n",
    "    \n",
    "    # If the input type can be converted to the output type directly\n",
    "    if output_type in transfer_matrix[input_type]:\n",
    "        return [[tool] for tool in transfer_matrix[input_type][output_type]]\n",
    "    \n",
    "    else:\n",
    "        if exclusive is None:\n",
    "            exclusive = set()\n",
    "        new_exclusive = exclusive.union({input_type})\n",
    "        \n",
    "        shortest_paths = []\n",
    "        for available_output_type, tools in transfer_matrix[input_type].items():\n",
    "            # If the output type is not in the exclusive list\n",
    "            if available_output_type not in new_exclusive:\n",
    "                # If the output type can be converted to the target output type\n",
    "                paths = find_path(available_output_type, output_type, new_exclusive)\n",
    "                if paths != []:\n",
    "                    paths = [[tool] + path for tool in tools for path in paths]\n",
    "                    shortest_paths.extend(paths)\n",
    "        \n",
    "        if shortest_paths:\n",
    "            # Filter the paths to keep the shortest ones\n",
    "            min_length = min([len(path) for path in shortest_paths])\n",
    "            shortest_paths = [path for path in shortest_paths if len(path) == min_length]\n",
    "        \n",
    "        return shortest_paths\n",
    "\n",
    "\n",
    "find_path(\"FASTA_PATH\", \"AA_SEQUENCE_LIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e48be967a2f879c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T10:12:16.698890500Z",
     "start_time": "2025-04-09T10:12:16.656120900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_query(tool_chain: list, required_inputs: set) -> str:\n",
    "    prompt = \"\"\"\\\n",
    "    You are a helpful AI assistant. Your task is to generate a reasonable user query based on a given tool chain so that this query could be answered after the tool chain is executed successfully.\n",
    "    \n",
    "    The description of each tool is provided below:\n",
    "    PUT_TOOL_DESCRIPTION_HERE\n",
    "    \n",
    "    The order of the tools is provided below:\n",
    "    PUT_TOOL_ORDER_HERE\n",
    "    \n",
    "    The inputs you should provide are:\n",
    "    PUT_TOOL_INPUT_HERE\n",
    "    \n",
    "    You have to generate a query like a normal user. The query should be a natural language question or command that reflect what the user wants to do, usually regarding a real-world scenario. The query should be clear and concise. The query should contain the inputs you provided, not the keys but only the values. And you should not directly mention the tool names in your query.\n",
    "    \n",
    "    Example:\n",
    "    1. Predict the structure of the sequence \"AEGIKL\", and the calculate the tmscore between this structure and \"/example.pdb\".\n",
    "    \n",
    "    2. Can you fetch the 3D structural data for the protein with UniProt ID P05067 and then align it with the structure in \"/example_2.cif\" to determine the TM-score?\n",
    "    \n",
    "    Now, please generate a user query based on the above information. Directly output the generated query.\n",
    "    \"\"\"\n",
    "    tool_desc = tool_manager.brief_documents(tool_chain)\n",
    "    \n",
    "    tool_order = \" -> \".join(tool_chain)\n",
    "    user_input =  \"\\n\".join([f\"{key}: '{type2case[key]}'\" for key in required_inputs])\n",
    "    prompt = prompt.replace(\"PUT_TOOL_DESCRIPTION_HERE\", tool_desc).replace(\"PUT_TOOL_ORDER_HERE\", tool_order).replace(\"PUT_TOOL_INPUT_HERE\", user_input)\n",
    "    \n",
    "    client = OpenAI(\n",
    "        api_key=\"sk-QzZzg2fubuHu7DXQQnAKBSN2o1hpU4MipzgmoVpWpZC1ODxs\",\n",
    "        base_url=\"https://api.kwwai.top/v1\"\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"claude-3-7-sonnet-20250219\",  \n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "adab06714aee26f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:24:35.235679200Z",
     "start_time": "2025-04-09T11:23:56.258292700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:53<00:00,  5.36s/it]\n"
     ]
    }
   ],
   "source": [
    "for key, case_dict in tqdm(cases.items()):\n",
    "    query = generate_query(case_dict[\"tool_chain\"], case_dict[\"required_inputs\"])\n",
    "    cases[key][\"query\"] = query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "215b8b72aee4bcce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:24:47.671329300Z",
     "start_time": "2025-04-09T11:24:47.621727900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 51025.60it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "save_path = \"/home/public/ProtAgent/agent_testset0729.tsv\"\n",
    "with open(save_path, \"a\", newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter='\\t', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    # writer.writerow([\n",
    "    #     \"tool_chain\",\n",
    "    #     \"required_inputs\",\n",
    "    #     \"query\"\n",
    "    # ])\n",
    "    for key, case_dict in tqdm(cases.items()):\n",
    "        writer.writerow([\n",
    "            key,\n",
    "            case_dict['required_inputs'],\n",
    "            case_dict['query']\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a84c4211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(save_path, sep='\\t')\n",
    "\n",
    "# Drop duplicated row[0]\n",
    "df = df.drop_duplicates(subset=[df.columns[0]])\n",
    "\n",
    "df.to_csv(f\"{save_path}.unique.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d5d3985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_df = pd.read_csv(\"/home/public/ProtAgent/agent_testset0729.tsv\", sep='\\t')\n",
    "\n",
    "def remove_condition(str):\n",
    "    if \"saprot_tune\" in str:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "# remove rows that column[0] contains \"saprot_tune\"\n",
    "origin_df_filtered = origin_df[~origin_df.iloc[:, 0].apply(remove_condition)]\n",
    "origin_df_filtered.to_csv(\"/home/public/ProtAgent/agent_testset0729.removed_saprot.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "724a42e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = pd.read_csv(\"/home/public/ProtAgent/agent_testset0728.tsv.saprot.tsv\")\n",
    "df_filtered.drop_duplicates()\n",
    "df_filtered.to_csv(\"/home/public/ProtAgent/agent_testset0728.tsv.saprot.unique.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ce80b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chains = df_filtered.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b91f9272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_case_by_chain(chain):\n",
    "    # Split the chain by \" -> \"\n",
    "    tool_chain = chain.split(\" -> \")\n",
    "    \n",
    "    # Get the required inputs\n",
    "    required_inputs = set()\n",
    "    for tool in tool_chain:\n",
    "        required_inputs = required_inputs.union(set(tools[tool][\"input_types\"]))\n",
    "    \n",
    "    # Get the query\n",
    "    query = generate_query(tool_chain, required_inputs)\n",
    "    \n",
    "    return {\n",
    "        \"required_inputs\": required_inputs,\n",
    "        \"query\": query\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "42a13887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saprot_tuned_inference_token_classification'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_chains[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0dbfa3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [05:19<00:00,  5.07s/it]\n"
     ]
    }
   ],
   "source": [
    "cases = {}\n",
    "for chain in tqdm(new_chains):\n",
    "    chain = chain[0]\n",
    "    cases[chain] = gen_case_by_chain(chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protagent_backbone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
